{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "\n",
    "# Generate regression toy data\n",
    "n_samples = 1000\n",
    "n_features = 5\n",
    "\n",
    "X, y = make_regression(n_samples=n_samples, n_features=n_features, random_state=42)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "train_ratio = 0.8\n",
    "train_size = int(train_ratio * n_samples)\n",
    "\n",
    "X_train = X[:train_size]\n",
    "y_train = y[:train_size]\n",
    "\n",
    "# Test data\n",
    "X_test = X[train_size:]\n",
    "y_test = y[train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "\n",
    "def bo_params_generic(model, params, X_train, y_train):\n",
    "    # Create the model instance with the specified parameters\n",
    "    regressor = model(**params)\n",
    "    \n",
    "    # Assuming you have X_train, y_train defined for regression\n",
    "    scores = cross_val_score(regressor, X_train, y_train, cv=10, scoring='neg_root_mean_squared_error')\n",
    "    return -scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#results = dt_bo.maximize(n_iter=5, init_points=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_models = {}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLR"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**feature selection**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Coefficients:\n",
      "Feature 2: 46.07121713482753\n",
      "Feature 3: 28.6279862111941\n",
      "Feature 4: 24.74629812331462\n",
      "Feature 1: 18.993474366101992\n",
      "Feature 0: 16.823657910849178\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Assuming you have X_train and y_train defined for training data\n",
    "\n",
    "# Initialize the Linear Regression model\n",
    "lr_model = LinearRegression()\n",
    "\n",
    "# Fit the Linear Regression model to the training data\n",
    "lr_model.fit(X_train, y_train)\n",
    "\n",
    "# Get feature coefficients\n",
    "coefficients = lr_model.coef_\n",
    "\n",
    "# Create a list of feature names or indices paired with their coefficients\n",
    "feature_coefficients = list(zip(range(X_train.shape[1]), coefficients))\n",
    "\n",
    "# Sort the features based on absolute coefficient values in descending order\n",
    "feature_coefficients.sort(key=lambda x: abs(x[1]), reverse=True)\n",
    "\n",
    "# Print the ranked feature coefficients\n",
    "print(\"Feature Coefficients:\")\n",
    "for feature_index, coefficient in feature_coefficients:\n",
    "    print(f\"Feature {feature_index}: {coefficient}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hyperparameter optimalisatie**\n",
    "#heb deze even uitgecomment omdat hij breekt als er geen min en max values zijn gedefinieerd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import subprocess\n",
    "# import sys\n",
    "\n",
    "# def install(package):\n",
    "    # subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "\n",
    "# install(\"bayesian-optimization\")\n",
    "# from sklearn.linear_model import LinearRegression\n",
    "# from bayes_opt import BayesianOptimization\n",
    "\n",
    "# params_ranges = {\n",
    "    # Define the parameter ranges for MLR\n",
    "    # 'param1': (value_min, value_max),\n",
    "    # 'param2': (value_min, value_max),\n",
    "    \n",
    "# }\n",
    "\n",
    "# Example usage with MLR\n",
    "# model = LinearRegression\n",
    "# dt_bo = BayesianOptimization(f=lambda param1, param2, :\n",
    "                                    # bo_params_generic(model, {\n",
    "                                        # 'param1': param1,\n",
    "                                        # 'param2': param2,\n",
    "                                        \n",
    "                                    # }, X_train, y_train),\n",
    "                            #  pbounds=params_ranges)\n",
    "# results = dt_bo.maximize(n_iter=5, init_points=20)\n",
    "# params = dt_bo.max['params']\n",
    "\n",
    "# Creating a model with the best hyperparameters\n",
    "# best_model = model()\n",
    "# best_model.set_params(**params)\n",
    "\n",
    "# Fit the model\n",
    "# best_model.fit(X_train, y_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machines"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Feature selection**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Importances:\n",
      "Feature 2: 0.5100907245192264\n",
      "Feature 3: 0.1812236125303029\n",
      "Feature 4: 0.13843292474891247\n",
      "Feature 1: 0.10140750241353605\n",
      "Feature 0: 0.07508208211413692\n"
     ]
    }
   ],
   "source": [
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "# Assuming you have X_train and y_train defined for training data\n",
    "\n",
    "# Initialize the SVM model\n",
    "svm_model = SVR(kernel='rbf')  # Replace 'rbf' with your desired kernel\n",
    "\n",
    "# Fit the SVM model to the training data\n",
    "svm_model.fit(X_train, y_train)\n",
    "\n",
    "# Compute permutation importances\n",
    "result = permutation_importance(svm_model, X_train, y_train, n_repeats=10, random_state=42)\n",
    "\n",
    "# Get feature importances\n",
    "importances = result.importances_mean\n",
    "\n",
    "# Create a list of feature names or indices paired with their importances\n",
    "feature_importances = list(zip(range(X_train.shape[1]), importances))\n",
    "\n",
    "# Sort the features based on importance in descending order\n",
    "feature_importances.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Print the ranked feature importances\n",
    "print(\"Feature Importances:\")\n",
    "for feature_index, importance in feature_importances:\n",
    "    print(f\"Feature {feature_index}: {importance}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Feature selection**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Assuming you have X_train and y_train defined for training data\n",
    "\n",
    "# Initialize the Random Forest model\n",
    "rf_model = RandomForestRegressor()\n",
    "\n",
    "# Fit the Random Forest model to the training data\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Get feature importances\n",
    "importances = rf_model.feature_importances_\n",
    "\n",
    "# Create a list of feature names or indices paired with their importances\n",
    "feature_importances = list(zip(range(X_train.shape[1]), importances))\n",
    "\n",
    "# Sort the features based on importance in descending order\n",
    "feature_importances.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Print the ranked feature importances\n",
    "print(\"Feature Importances:\")\n",
    "for feature_index, importance in feature_importances:\n",
    "    print(f\"Feature {feature_index}: {importance}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hyperparameter optimalisatie**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def install(package):\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "\n",
    "install(\"bayesian-optimization\")\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from bayes_opt import BayesianOptimization\n",
    "\n",
    "params_ranges = {\n",
    "    'n_estimators': (10, 100),\n",
    "    'max_depth': (1, 20),\n",
    "    'min_samples_leaf': (1, 10),\n",
    "    'min_weight_fraction_leaf': (0.0, 0.5),\n",
    "    'max_features': (0.1, 1),\n",
    "    'max_leaf_nodes': (10, 100)\n",
    "}\n",
    "\n",
    "# Example usage with Random Forest\n",
    "model = RandomForestRegressor\n",
    "dt_bo = BayesianOptimization(f=lambda n_estimators, max_depth, min_samples_leaf, min_weight_fraction_leaf,\n",
    "                                    max_features, max_leaf_nodes: bo_params_generic(model, {\n",
    "                                        'n_estimators': int(round(n_estimators)),\n",
    "                                        'max_depth': int(round(max_depth)),\n",
    "                                        'min_samples_leaf': round(min_samples_leaf),\n",
    "                                        'min_weight_fraction_leaf': min_weight_fraction_leaf,\n",
    "                                        'max_features': max_features,\n",
    "                                        'max_leaf_nodes': int(round(max_leaf_nodes))\n",
    "                                    }, X_train, y_train),\n",
    "                             pbounds=params_ranges)\n",
    "results = dt_bo.maximize(n_iter=5, init_points=20)\n",
    "params = dt_bo.max['params']\n",
    "\n",
    "# Creating a model with the best hyperparameters\n",
    "best_model = model(\n",
    "    n_estimators=int(round(params['n_estimators'])),\n",
    "    max_depth=int(round(params['max_depth'])),\n",
    "    min_samples_leaf=round(params['min_samples_leaf']),\n",
    "    min_weight_fraction_leaf=params['min_weight_fraction_leaf'],\n",
    "    max_features=params['max_features'],\n",
    "    max_leaf_nodes=int(round(params['max_leaf_nodes']))\n",
    ")\n",
    "\n",
    "# Fit the model\n",
    "best_model.fit(X_train, y_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# neural network"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Feature selection**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "# Assuming you have X and y defined for the dataset\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale the input features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Initialize the neural network regressor\n",
    "nn_model = MLPRegressor(hidden_layer_sizes=(10, 10))  # Adjust the architecture as needed\n",
    "\n",
    "# Fit the neural network model to the training data\n",
    "nn_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Perform feature selection using Recursive Feature Elimination (RFE)\n",
    "selector = RFE(estimator=nn_model, n_features_to_select=10)  # Adjust n_features_to_select as needed\n",
    "selector.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Transform the training and testing sets to keep only the selected features\n",
    "X_train_selected = selector.transform(X_train_scaled)\n",
    "X_test_selected = selector.transform(X_test_scaled)\n",
    "\n",
    "# Print the selected feature support\n",
    "selected_support = selector.support_\n",
    "print(\"Selected Feature Support:\")\n",
    "print(selected_support)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hyperparameter optimalisatie**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def install(package):\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "\n",
    "install(\"bayesian-optimization\")\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from bayes_opt import BayesianOptimization\n",
    "\n",
    "params_ranges = {\n",
    "    'hidden_layer_sizes': (10, 100),\n",
    "    'alpha': (0.0001, 0.1),\n",
    "    'learning_rate_init': (0.001, 0.1),\n",
    "    'max_iter': (100, 1000),\n",
    "}\n",
    "\n",
    "# Example usage with Neural Network\n",
    "model = MLPRegressor\n",
    "dt_bo = BayesianOptimization(f=lambda hidden_layer_sizes, alpha, learning_rate_init, max_iter:\n",
    "                                    bo_params_generic(model, {\n",
    "                                        'hidden_layer_sizes': (int(round(hidden_layer_sizes)),),\n",
    "                                        'alpha': alpha,\n",
    "                                        'learning_rate_init': learning_rate_init,\n",
    "                                        'max_iter': int(round(max_iter))\n",
    "                                    }, X_train, y_train),\n",
    "                             pbounds=params_ranges)\n",
    "\n",
    "results = dt_bo.maximize(n_iter=5, init_points=20)\n",
    "params = dt_bo.max['params']\n",
    "\n",
    "# Creating a model with the best hyperparameters\n",
    "best_model = model(\n",
    "    hidden_layer_sizes=(int(round(params['hidden_layer_sizes'])),),\n",
    "    alpha=params['alpha'],\n",
    "    learning_rate_init=params['learning_rate_init'],\n",
    "    max_iter=int(round(params['max_iter']))\n",
    ")\n",
    "\n",
    "# Fit the model\n",
    "best_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append model to list\n",
    "all_models['Random forest'] = best_model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
